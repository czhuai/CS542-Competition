{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1681782164424,"user":{"displayName":"Chen ZHU","userId":"01644350571762964564"},"user_tz":240},"id":"AwQNvgTxTl7Y"},"outputs":[],"source":["import os\n","import json\n","import pickle\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1235,"status":"ok","timestamp":1681782165657,"user":{"displayName":"Chen ZHU","userId":"01644350571762964564"},"user_tz":240},"id":"G0jLrfc0U99K","outputId":"01d4bafd-1484-4533-9077-5e60d97ba0e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import os\n","os.chdir(\"/content/gdrive/MyDrive/CS542/cs542-autocast/\")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":16154,"status":"ok","timestamp":1681782181809,"user":{"displayName":"Chen ZHU","userId":"01644350571762964564"},"user_tz":240},"id":"f5ompYhpTl7a"},"outputs":[],"source":["autocast_questions = json.load(open('autocast_questions.json')) # from the Autocast dataset\n","test_questions = json.load(open('autocast_competition_test_set.json'))\n","test_ids = [q['id'] for q in test_questions]\n","test_types = [q['qtype'] for q in test_questions]"]},{"cell_type":"markdown","metadata":{"id":"1K-kIT3_Tl7a"},"source":["## Create baseline models outputting random answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BaB4vuveTl7b"},"outputs":[],"source":["def random_baseline_model(question):\n","    if question['qtype'] == 't/f':\n","        return np.random.random(size=2)\n","    elif question['qtype'] == 'mc':\n","        probs = np.random.random(size=len(question['choices']))\n","        return probs / probs.sum()\n","    elif question['qtype'] == 'num':\n","        return np.random.random()\n","\n","\n","def calibrated_random_baseline_model(question):\n","    if question['qtype'] == 't/f':\n","        pred_idx = np.argmax(np.random.random(size=2))\n","        pred = np.ones(2)\n","        pred[pred_idx] += 1e-5\n","        return pred / pred.sum()\n","    elif question['qtype'] == 'mc':\n","        pred_idx = np.argmax(np.random.random(size=len(question['choices'])))\n","        pred = np.ones(len(question['choices']))\n","        pred[pred_idx] += 1e-5\n","        return pred / pred.sum()\n","    elif question['qtype'] == 'num':\n","        return 0.5"]},{"cell_type":"markdown","metadata":{"id":"yldlEEqiTl7b"},"source":["## Get performance on the Autocast train set\n","\n","Note that the Autocast dataset contains questions in the competition test set. Those should not be used."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twsmtWIUTl7b"},"outputs":[],"source":["def brier_score(probabilities, answer_probabilities):\n","    return ((probabilities - answer_probabilities) ** 2).sum() / 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GtsmZ6WsTl7c"},"outputs":[],"source":["preds = []\n","answers = []\n","qtypes = []\n","for question in autocast_questions:\n","    if question['id'] in test_ids: # skipping questions in the competition test set\n","        continue\n","    if question['answer'] is None: # skipping questions without answer\n","        continue\n","    preds.append(calibrated_random_baseline_model(question))\n","    if question['qtype'] == 't/f':\n","        ans_idx = 0 if question['answer'] == 'no' else 1\n","        ans = np.zeros(len(question['choices']))\n","        ans[ans_idx] = 1\n","        qtypes.append('t/f')\n","    elif question['qtype'] == 'mc':\n","        ans_idx = ord(question['answer']) - ord('A')\n","        ans = np.zeros(len(question['choices']))\n","        ans[ans_idx] = 1\n","        qtypes.append('mc')\n","    elif question['qtype'] == 'num':\n","        ans = float(question['answer'])\n","        qtypes.append('num')\n","    answers.append(ans)"]},{"cell_type":"markdown","metadata":{"id":"W0B8kcYgTl7c"},"source":["## Evaluate the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTeT-sASTl7c"},"outputs":[],"source":["tf_results, mc_results, num_results = [],[],[]\n","for p, a, qtype in zip(preds, answers, qtypes):\n","    if qtype == 't/f':\n","        tf_results.append(brier_score(p, a))\n","    elif qtype == 'mc':\n","        mc_results.append(brier_score(p, a))\n","    else:\n","        num_results.append(np.abs(p - a))\n","\n","print(f\"T/F: {np.mean(tf_results)*100:.2f}, MCQ: {np.mean(mc_results)*100:.2f}, NUM: {np.mean(num_results)*100:.2f}\")\n","print(f\"Combined Metric: {(np.mean(tf_results) + np.mean(mc_results) + np.mean(num_results))*100:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"yNNQQharTl7d"},"source":["## Make predictions on test set"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29456,"status":"ok","timestamp":1681782246201,"user":{"displayName":"Chen ZHU","userId":"01644350571762964564"},"user_tz":240},"id":"YIVfZN58_-WK","outputId":"d6583b0b-8d77-4b6f-e1f8-5885b4fd1fa7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.98\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.11.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement copy (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for copy\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install torch\n","!pip install copy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GU902ZfA4Na2"},"outputs":[],"source":["import regex\n","import string\n","import transformers\n","\n","import time\n","import sys\n","import torch\n","import pickle\n","from tqdm import tqdm\n","from torch._C import TensorType\n","import torch.nn.functional as F\n","import numpy as np\n","from pathlib import Path\n","from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import torch.distributed as dist\n","import copy\n","from torch.nn import CrossEntropyLoss, MSELoss\n","from torch import nn\n","\n","sys.path.append(\"\")\n","from ./autocast.cs542.src.options import Options\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQBC9UjQ4JaV"},"outputs":[],"source":["def normalize_answer(s):\n","    def remove_articles(text):\n","        return regex.sub(r'\\b(a|an|the)\\b', ' ', text)\n","\n","    def white_space_fix(text):\n","        return ' '.join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return ''.join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeDxppE8_6pp"},"outputs":[],"source":["model_class = transformers.T5ForConditionalGeneration\n","model_path = \"\"\n","epoch_path = os.path.realpath(model_path)\n","model = model_class.from_pretrained(epoch_path)\n","model = model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vhp31nMAAKPU"},"outputs":[],"source":["def evaluate(model, dataset, tokenizer, collator, opt, epoch, device, mode='eval'):\n","    # TF_TOKENS = sum(tokenizer(['no', 'yes'])['input_ids'], [])\n","    # MC_TOKENS = sum(tokenizer([chr(i + ord('A')) for i in range(12)])['input_ids'], [])\n","\n","    sampler = SequentialSampler(dataset)\n","    dataloader = DataLoader(dataset,\n","                            sampler=sampler,\n","                            batch_size=opt.per_gpu_batch_size,\n","                            drop_last=False,\n","                            # num_workers=2,\n","                            collate_fn=collator\n","                            )\n","    model.eval()\n","    total = 0\n","    tf_em, mc_em, re_em, exactmatch = [], [], [], []\n","    tf_predictions, mc_predictions, re_predictions, my_predictions = [], [], [], []\n","    model = model.module if hasattr(model, \"module\") else model\n","    cpu_device = torch.device('cpu')\n","    raw_logits, qids, raw_answers = [], [], []\n","    with torch.no_grad():\n","        pbar = tqdm(dataloader, total=len(dataloader))\n","        for i, batch in enumerate(pbar):\n","            (idx, ids, labels, indices, lengths, context_ids, context_mask) = batch\n","\n","            labels = labels.to(device)\n","            indices = indices.to(device)\n","            lengths = lengths.to(device)\n","            input_ids = context_ids.to(device)\n","            input_ids = input_ids.view(input_ids.size(0), -1)\n","            attention_mask = context_mask.to(device)\n","            attention_mask = attention_mask.view(attention_mask.size(0), -1)\n","\n","            indices_tfmc = indices[0][:lengths[0]]\n","            indices_re = indices[1][:lengths[1]]\n","            labels_tfmc, labels_re = None, None\n","\n","            if labels is None:\n","                decoder_outputs = model.forward(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    labels=labels,\n","                    output_hidden_states=True,\n","                )\n","                hidden_state = decoder_outputs[2][-1]\n","                previous_outputs = decoder_outputs[1]\n","                logits = decoder_outputs[0]\n","            else:\n","                labels_tfmc = torch.index_select(labels, 0, indices_tfmc).to(torch.int64)\n","                labels_re = torch.index_select(labels, 0, indices_re)\n","\n","                decoder_labels = copy.deepcopy(labels).to(torch.int64)\n","                decoder_labels[indices_re, :] = torch.zeros_like(labels_re).to(torch.int64).to(device)\n","                labels_re = labels_re[:, 0].view(-1, 1)  # only takes the first value, as all others are copies\n","                decoder_outputs = model.forward(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    labels=decoder_labels,\n","                    output_hidden_states=True,\n","                )\n","                hidden_state = decoder_outputs[3][-1]\n","                previous_outputs = decoder_outputs[2]\n","                logits = decoder_outputs[1]\n","\n","            # raw_logits.append(logits)\n","            regressor = nn.Sequential(\n","                nn.Linear(model.config.d_model, 1),\n","                nn.Sigmoid()\n","            )\n","\n","            regressor = regressor.to(device)\n","\n","            results_re = torch.index_select(regressor(hidden_state)[:, 0, :], 0, indices_re)\n","\n","            if labels is None:\n","                return logits, previous_outputs, None, results_re\n","\n","            loss_fn_classifier, loss_fn_regressor = CrossEntropyLoss(ignore_index=-100), MSELoss()\n","            loss_tfmc, loss_re = torch.tensor(0.0).cuda(), torch.tensor(0.0).cuda()\n","\n","            re_outputs = results_re.view(-1, results_re.size(-1))\n","            \n","            tfmc_outputs = model.generate(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                max_length=10\n","            )\n","            \n","            indices_re = indices[1][:lengths[1]]\n","            indices_tf = indices[2][:lengths[2]]\n","            indices_mc = indices[3][:lengths[3]]\n","\n","            labels_re = torch.index_select(labels, 0, indices_re)[:, 0].view(-1).detach().to(cpu_device).tolist()\n","\n","            tf_scores, mc_scores = [], []\n","            # tf_logits, mc_logits = [], []\n","            tf_ans, mc_ans = [], []\n","            \n","            ans_list = []\n","        \n","            # for k, (o, lgs) in enumerate(zip(tfmc_outputs, output_logits)):\n","            for k, o in enumerate(tfmc_outputs):\n"," \n","                ans = tokenizer.decode(o, skip_special_tokens=True)\n","                \n","                gold = [str(dataset.get_example(idx[k])['answer'])]\n","                score = src.evaluation.ems(ans, gold)\n","                total += 1\n","\n","                if k in indices_tf:\n","                    tf_scores.append(score)\n","                    tf_em.append(score)\n","                    tf_ans.append(ans)\n","                    tf_predictions.append(ans)\n","                    ans_list.append(src.evaluation.normalize_answer(ans))\n","\n","                elif k in indices_mc:\n","                    mc_scores.append(score)\n","                    mc_em.append(score)\n","                    mc_ans.append(ans)\n","                    mc_predictions.append(ans)\n","                    ans_list.append(src.evaluation.normalize_answer(ans))\n","\n","            re_ans = []\n","            if len(labels_re) > 0:\n","                re_ans = re_outputs.view(-1).detach().to(cpu_device).tolist()\n","                for item in re_ans:\n","                    ans_list.append(item)\n","            re_scores = [np.abs(re_ans[i] - labels_re[i]) \\\n","                         for i in range(len(labels_re))]\n","            total += len(re_scores)\n","            re_predictions.extend(re_ans)\n","            re_em.extend(re_scores)\n","\n","            temp_scores, temp_predictions = [], []\n","            tf_count, mc_count, re_count = 0, 0, 0\n","            re_outputs = re_outputs.to(cpu_device).tolist()\n","            for i in range(len(idx)):\n","                if i in indices_tf:\n","                    temp_scores.append(tf_scores[tf_count])\n","                    if mode == 'eval':\n","                        temp_predictions.append(tf_ans[tf_count])\n","                        # raw_logits.append(tf_logits[tf_count])\n","                    tf_count += 1\n","                elif i in indices_mc:\n","                    temp_scores.append(mc_scores[mc_count])\n","                    if mode == 'eval':\n","                        temp_predictions.append(mc_ans[mc_count])\n","                        # raw_logits.append(mc_logits[mc_count])\n","                    mc_count += 1\n","                elif i in indices_re:\n","                    temp_scores.append(-re_scores[re_count])\n","                    if mode == 'eval':\n","                        temp_predictions.append(re_ans[re_count])\n","                        # raw_logits.append(re_outputs[re_count])\n","                    re_count += 1\n","                qids.append(ids[i])\n","                raw_answers.append(str(dataset.get_example(idx[i])['answer']))\n","\n","            exactmatch.extend(temp_scores)\n","            my_predictions.extend(temp_predictions)\n","\n","    if opt.is_distributed:\n","        # objects = [tf_em, mc_em, re_em, tf_predictions, mc_predictions, re_predictions, raw_logits, qids, raw_answers]\n","        objects = [tf_em, mc_em, re_em, tf_predictions, mc_predictions, re_predictions, qids, raw_answers]\n","        all_objects = [None for _ in range(opt.world_size)]\n","        dist.gather_object(objects, all_objects if dist.get_rank() == 0 else None)\n","\n","        if opt.is_main:\n","            main_list = [[] for _ in range(len(objects))]\n","            for rank, obj_list in enumerate(all_objects):\n","                for i, obj in enumerate(obj_list):\n","                    main_list[i] += obj  # extend list to gather\n","            # tf_em, mc_em, re_em, tf_predictions, mc_predictions, re_predictions, raw_logits, qids, raw_answers = main_list\n","            tf_em, mc_em, re_em, tf_predictions, mc_predictions, re_predictions, qids, raw_answers = main_list\n","\n","    if mode == 'eval' and (not opt.is_distributed or opt.is_main):\n","        if len(tf_em) == 0:\n","            logger.info(f\"EVAL: For T/F: Predicted N/A\")\n","        else:\n","            logger.info(f\"EVAL: For T/F: Predicted {tf_em.count(1)} Match {tf_em.count(0)} Wrong \\\n","            ({tf_predictions.count('yes')} YES {tf_predictions.count('no')} NO) | EM: {round(tf_em.count(1) / len(tf_em) * 100, 2)}\")\n","        if len(mc_em) == 0:\n","            logger.info(f\"       For MC:  Predicted N/A\")\n","        else:\n","            logger.info(f\"       For MC:  Predicted {mc_em.count(1)} Match {mc_em.count(0)} Wrong | \\\n","            EM: {round(mc_em.count(1) / len(mc_em) * 100, 2)}\")\n","        if len(re_em) == 0:\n","            logger.info(f\"       For Reg: Predicted N/A\")\n","        else:\n","            logger.info(f\"       For Reg: Dist {np.mean(re_em)}\")\n","\n","    if mode == 'train' and (not opt.is_distributed or opt.is_main):\n","        if len(tf_em) == 0:\n","            logger.info(f\"TRAIN: For T/F: Predicted N/A\")\n","        else:\n","            logger.info(f\"TRAIN: For T/F: Predicted {tf_em.count(1)} Match {tf_em.count(0)} Wrong \\\n","            ({tf_predictions.count('yes')} YES {tf_predictions.count('no')} NO) | EM: {round(tf_em.count(1) / len(tf_em) * 100, 2)}\")\n","        if len(mc_em) == 0:\n","            logger.info(f\"       For MC:  Predicted N/A\")\n","        else:\n","            logger.info(f\"       For MC:  Predicted {mc_em.count(1)} Match {mc_em.count(0)} Wrong | \\\n","            EM: {round(mc_em.count(1) / len(mc_em) * 100, 2)}\")\n","        if len(re_em) == 0:\n","            logger.info(f\"       For Reg: Predicted N/A\")\n","        else:\n","            logger.info(f\"       For Reg: Dist {np.mean(re_em)}\")\n","\n","    if mode == 'eval' and (not opt.is_distributed or opt.is_main):\n","        with open(checkpoint_path / f'results_epoch{epoch}.obj', 'wb') as f:\n","            # pickle.dump(list(zip(qids, raw_answers, raw_logits)), f)\n","            pickle.dump(list(zip(qids, raw_answers)), f)\n","\n","    exactmatch, total = src.util.weighted_average(np.mean(exactmatch) / 2, total, opt)\n","    return exactmatch, ans_list\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ol3vLvjSTl7d"},"outputs":[],"source":["preds = []\n","for question in test_questions:\n","    result = \"...\"\n","    if question['qtype'] == 't/f':\n","        result = normalize_answer(str(result))\n","        if result == \"yes\":\n","            preds.append([0, 1])\n","        elif result == \"no\":\n","            preds.append([1, 0])\n","        else:\n","            pred_idx = np.argmax(np.random.random(size=2))\n","            pred = np.ones(2)\n","            pred[pred_idx] += 1e-5\n","            preds.append(pred / pred.sum())\n","    elif question['qtype'] == 'mc':\n","        result = normalize_answer(str(result))\n","        try:\n","            pred = np.zeros(len(question['choices']))\n","            ans_idx = ord(result) - ord('a')\n","            pred[ans_idx] = 1\n","            preds.append(pred)\n","        except:\n","          pred_idx = np.argmax(np.random.random(size=len(question['choices'])))\n","          pred = np.ones(len(question['choices']))\n","          pred[pred_idx] += 1e-5\n","          preds.append(pred / pred.sum())\n","    elif question['qtype'] == 'num':\n","        try:\n","            ans = float(result)\n","            preds.append(ans)\n","        except:\n","            preds.append(0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3xRK-FXTl7e","outputId":"f5f2b0a5-6e67-4d23-ba38-39e409398545"},"outputs":[{"name":"stdout","output_type":"stream","text":["updating: predictions.pkl (deflated 79%)\n"]}],"source":["if not os.path.exists('submission'):\n","    os.makedirs('submission')\n","\n","with open(os.path.join('submission', 'predictions.pkl'), 'wb') as f:\n","    pickle.dump(preds, f, protocol=2)\n","\n","!cd submission && zip ../submission.zip ./* && cd .."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEBaBGQ3Tl7e","outputId":"c169872c-c69d-467f-8afa-06350aabe51a"},"outputs":[{"name":"stdout","output_type":"stream","text":["autocast_competition_test_set.json \u001b[36msubmission\u001b[m\u001b[m\n","autocast_questions.json            submission.zip\n","example_submission.ipynb\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_tIiYnoTl7e"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.7.4 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"90e47ca3de7a6ac5652c507781a9a883127089d6067d2cae315ebae4b66e7ceb"}}},"nbformat":4,"nbformat_minor":0}
